}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
errors[p] = mean(abs((mean-real)/real))
p=p+1
nrow(train)
train
adddata
add = 2
adddata=data[index[1:add],1:3]
adddata
dim(data)
dim(train)
train=t(data.frame(t(train),t(adddata)))
dim(train)
train
add = 1
adddata=data[index[1:add],1:3]
adddata
dim(adddata)
adddata=data[index[1:add],1:3,drop=FALSE]
dim(adddata)
setwd("/media/wxk/My Passport/OED_linux/src/Entropy")
source("../setpath.R")
library(mlegp)
# args = commandArgs(TRUE)
# add = as.integer(args[1])
# iter_num = as.integer(args[2])
add = 1
iter_num = 120
data= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic1.csv"), header=TRUE))
train= read.csv(file.path(out_data_path,"entropy_train.csv"), header=TRUE)
# bencharmark set
dat= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic2.csv"), header=TRUE))
errors = as.numeric()
ptm <- proc.time()
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
errors[p] = mean(abs((mean-real)/real))
p=p+1
nrow(train)
setwd("/media/wxk/My Passport/OED_linux/src/Entropy")
source("../setpath.R")
library(mlegp)
# args = commandArgs(TRUE)
# add = as.integer(args[1])
# iter_num = as.integer(args[2])
add = 1
iter_num = 120
data= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic1.csv"), header=TRUE))
train= read.csv(file.path(out_data_path,"entropy_train.csv"), header=TRUE)
# bencharmark set
dat= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic2.csv"), header=TRUE))
errors = as.numeric()
ptm <- proc.time()
for (p in 1:iter_num)
{
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
# select the top add rows and add them to train. (caveat: when indexing one row,keep the dimension)
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
errors[p] = mean(abs((mean-real)/real))
p=p+1
}
time=proc.time() -ptm
cat("time",time)
write.csv(errors,file=file.path(out_data_path,"errors","EN",paste("every_",add,"_",iter_num,"_iter.csv",sep="")))
errors
dim(train)
train = train[1:33,]
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
# select the top add rows and add them to train. (caveat: when indexing one row,keep the dimension)
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
mean(abs((mean-real)/real))
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
# select the top add rows and add them to train. (caveat: when indexing one row,keep the dimension)
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
mean(abs((mean-real)/real))
source("../setpath.R")
library(mlegp)
# args = commandArgs(TRUE)
# add = as.integer(args[1])
# iter_num = as.integer(args[2])
add = 1
iter_num = 120
data= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic1.csv"), header=TRUE))
train= read.csv(file.path(out_data_path,"entropy_train.csv"), header=TRUE)
# bencharmark set
dat= as.matrix(read.csv(file.path(out_data_path,"GPsynthetic2.csv"), header=TRUE))
errors = as.numeric()
ptm <- proc.time()
for (p in 1:iter_num)
{
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
# select the top add rows and add them to train. (caveat: when indexing one row,keep the dimension)
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
errors[p] = mean(abs((mean-real)/real))
p=p+1
}
time
errors
backup = train
nrow(train)
train = backup[1:52,]
xx=as.matrix(train[,1:2])
y=as.matrix(train[,3])
fit=mlegp(xx,y)
#fit=mlegp(xx,y,nugget=1,nugget.known=1)
inverse=fit$invVarMatrix
d=fit[["numDim"]]
beta=fit[["beta"]]
sigma=fit[["sig2"]]
mu=fit[["mu"]]
nugget=fit[["nugget"]]
number=fit[["numObs"]]
df=as.matrix(train)
newcov=matrix(nrow=dim(data)[1],ncol=number)
for ( i in 1:dim(data)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(data[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=data[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
index=sort(var, decreasing = TRUE,index.return=TRUE)$ix
# select the top add rows and add them to train. (caveat: when indexing one row,keep the dimension)
adddata=data[index[1:add],1:3,drop=FALSE]
train=t(data.frame(t(train),t(adddata)))
total=1:dim(data)[1]
remove=index[1:add]
left=total[!total%in%remove]
lefttrain=data[left,]
# update the pool
data = lefttrain
#test the prediction on benchmark set
# df is the train in current around
# dat is the benchmark set
newcov=matrix(nrow=dim(dat)[1],ncol=number)
i=1
for ( i in 1:dim(dat)[1])
{
j=1
for (j in 1:number)
{
k=1
dist=0
for (k in 1:d)
{
dist=dist-(dat[i,k]-df[j,k])^2*beta[k]
}
newcov[i,j]=sigma*exp(dist)
}
}
diag(newcov)=diag(newcov)+nugget
mean=newcov%*%inverse%*%(fit$Z - fit$mu)+mu[1]
real=dat[,3]
var=diag(sigma+nugget-newcov%*%inverse%*%t(newcov))
mean(abs((mean-real)/real))
train
errors
sample(1:20,3)
sample(1:20,3,replace = FALSE)
start_size = 20
select = sample(1:nrow(data), start_size)
left = data[-select,]
head(left)
dim(left)
data = read.csv(file.path(data_path,"surface.csv"), header=FALSE)
dim(data)
start_size = 20
select = sample(1:nrow(data), start_size)
write.csv(data[select,], file.path(out_data_path,"entropy_train.csv"),row.names=FALSE)
left = data[-select,]
dim(left)
random_seed=1
set.seed(random_seed)
data = read.csv(file.path(data_path,"surface.csv"), header=FALSE)
start_size = 20
select = sample(1:nrow(data), start_size)
select
random_seed=2
set.seed(random_seed)
data = read.csv(file.path(data_path,"surface.csv"), header=FALSE)
start_size = 20
select = sample(1:nrow(data), start_size)
select
random_seed=3
set.seed(random_seed)
data = read.csv(file.path(data_path,"surface.csv"), header=FALSE)
start_size = 20
select = sample(1:nrow(data), start_size)
select
